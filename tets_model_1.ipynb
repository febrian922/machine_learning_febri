{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "jumC4oqYVat1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from shutil import copyfile\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "D-IUOpGni8Og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_zip = '/content/drive/MyDrive/dataset/selada hijau.zip'\n",
        "zip_ref   = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/content/drive/MyDrive/dataset/selada_hijau/')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "ZC4GGoqOjW1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_path = '/content/drive/MyDrive/dataset/selada_hijau/FNNPK'\n",
        "\n",
        "Fully_Nutritional_Lettuce = os.path.join(source_path, 'FN')\n",
        "Nitrogen_Deficient_Lettuce = os.path.join(source_path, '-N')\n",
        "Phosphorus_Deficient_Lettuce = os.path.join(source_path, '-P')\n",
        "Potassium_Deficient_Lettuce = os.path.join(source_path, '-K')\n",
        "\n",
        "# Deletes all non-image files (there are two .db files bundled into the dataset)\n",
        "!find /content/drive/MyDrive/dataset/selada_hijau/FNNPK -type f ! -name \"*.png\" -exec rm {} +\n",
        "\n",
        "# os.listdir returns a list containing all files under the given path\n",
        "print(f\"There are {len(os.listdir(Fully_Nutritional_Lettuce))} FN Lettuce.\")\n",
        "print(f\"There are {len(os.listdir(Nitrogen_Deficient_Lettuce))} N Lettuce.\")\n",
        "print(f\"There are {len(os.listdir(Phosphorus_Deficient_Lettuce))} P Lettuce.\")\n",
        "print(f\"There are {len(os.listdir(Potassium_Deficient_Lettuce))}  k Lettuce.\")"
      ],
      "metadata": {
        "id": "yqc5-GL_jPaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FN_Lettuces_files = os.listdir(Fully_Nutritional_Lettuce)\n",
        "print(FN_Lettuces_files[:10])\n",
        "\n",
        "N_Lettuces_files = os.listdir(Nitrogen_Deficient_Lettuce)\n",
        "print(N_Lettuces_files[:10])\n",
        "\n",
        "P_Lettuces_files = os.listdir(Phosphorus_Deficient_Lettuce)\n",
        "print(P_Lettuces_files[:10])\n",
        "\n",
        "K_Lettuces_files = os.listdir(Potassium_Deficient_Lettuce)\n",
        "print(K_Lettuces_files[:10])"
      ],
      "metadata": {
        "id": "vOcxPVupnHwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "pic_index = 2\n",
        "\n",
        "Next_FN_Lettuce = [os.path.join(Fully_Nutritional_Lettuce, fname)\n",
        "                for fname in FN_Lettuces_files[pic_index-2:pic_index]]\n",
        "Next_N_Lettuce = [os.path.join(Nitrogen_Deficient_Lettuce, fname)\n",
        "                for fname in N_Lettuces_files[pic_index-2:pic_index]]\n",
        "Next_P_Lettuce = [os.path.join(Phosphorus_Deficient_Lettuce, fname)\n",
        "                for fname in P_Lettuces_files[pic_index-2:pic_index]]\n",
        "Next_K_Lettuce = [os.path.join(Potassium_Deficient_Lettuce, fname)\n",
        "                for fname in K_Lettuces_files[pic_index-2:pic_index]]\n",
        "\n",
        "for i, img_path in enumerate(Next_FN_Lettuce+Next_N_Lettuce+Next_P_Lettuce+Next_K_Lettuce):\n",
        "  img = mpimg.imread(img_path)\n",
        "  plt.imshow(img)\n",
        "  plt.axis('Off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "XDuxbo6ymwyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define root directory\n",
        "root_dir = '/content/drive/MyDrive/dataset/selada_hijau/train-val/'\n",
        "\n",
        "# Empty directory to prevent FileExistsError is the function is run several times\n",
        "if os.path.exists(root_dir):\n",
        "  shutil.rmtree(root_dir)"
      ],
      "metadata": {
        "id": "9c9bzqdnsBs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_train_val_dirs(root_path):\n",
        "\n",
        "  # train dir and val dir\n",
        "  train_dir = os.path.join(root_path,'training')\n",
        "  val_dir = os.path.join(root_path, 'validation')\n",
        "\n",
        "  os.makedirs(train_dir)\n",
        "  os.makedirs(val_dir)\n",
        "  # Create subdirectories for cats and dogs in the training set\n",
        "  train_FN_Lettuce_dir = os.path.join(train_dir, 'FN')\n",
        "  train_N_Lettuce_dir = os.path.join(train_dir, 'N')\n",
        "  train_P_Lettuce_dir = os.path.join(train_dir, 'P')\n",
        "  train_K_Lettuce_dir = os.path.join(train_dir, 'K')\n",
        "\n",
        "  os.makedirs(train_FN_Lettuce_dir)\n",
        "  os.makedirs(train_N_Lettuce_dir)\n",
        "  os.makedirs(train_P_Lettuce_dir)\n",
        "  os.makedirs(train_K_Lettuce_dir)\n",
        "\n",
        "  # Create subdirectories for cats and dogs in the validation set\n",
        "  val_FN_Lettuce_dir = os.path.join(val_dir, 'FN')\n",
        "  val_N_Lettuce_dir = os.path.join(val_dir, 'N')\n",
        "  val_P_Lettuce_dir = os.path.join(val_dir, 'P')\n",
        "  val_K_Lettuce_dir = os.path.join(val_dir, 'K')\n",
        "\n",
        "  os.makedirs(val_FN_Lettuce_dir)\n",
        "  os.makedirs(val_N_Lettuce_dir)\n",
        "  os.makedirs(val_P_Lettuce_dir)\n",
        "  os.makedirs(val_K_Lettuce_dir)\n",
        "\n",
        "try:\n",
        "  create_train_val_dirs(root_path=root_dir)\n",
        "except FileExistsError:\n",
        "  print(\"You should not be seeing this since the upper directory is removed beforehand\")"
      ],
      "metadata": {
        "id": "VP6LE7EZpR5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for rootdir, dirs, files in os.walk(root_dir):\n",
        "    for subdir in dirs:\n",
        "        print(os.path.join(rootdir, subdir))"
      ],
      "metadata": {
        "id": "uUU7-eC_uT2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(SOURCE_DIR, TRAINING_DIR, VALIDATION_DIR, SPLIT_SIZE):\n",
        "  all_files = []\n",
        "  file_list = os.listdir(SOURCE_DIR)\n",
        "\n",
        "  for file_name in file_list:\n",
        "    file_path = os.path.join(SOURCE_DIR, file_name)\n",
        "\n",
        "    file_size = os.path.getsize(file_path)\n",
        "    if file_size == 0 :\n",
        "      print('{} is zero length, so ignoring'.format(file_name))\n",
        "    else:\n",
        "      all_files.append(file_name)\n",
        "\n",
        "  shuffled_files = random.sample(all_files, len(all_files))\n",
        "  split_index = int(len(all_files) * SPLIT_SIZE)\n",
        "  training_files = shuffled_files[:split_index]\n",
        "  validation_files = shuffled_files[split_index:]\n",
        "\n",
        "  for file in training_files:\n",
        "    copyfile(os.path.join(SOURCE_DIR, file), os.path.join(TRAINING_DIR, file))\n",
        "\n",
        "  for file in validation_files:\n",
        "    copyfile(os.path.join(SOURCE_DIR, file), os.path.join(VALIDATION_DIR, file))\n"
      ],
      "metadata": {
        "id": "CkZ-PmYxtdIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FN_DIR = \"/content/drive/MyDrive/dataset/selada_hijau/FNNPK/FN/\"\n",
        "N_DIR = \"/content/drive/MyDrive/dataset/selada_hijau/FNNPK/-N/\"\n",
        "P_DIR = \"/content/drive/MyDrive/dataset/selada_hijau/FNNPK/-P/\"\n",
        "K_DIR = \"/content/drive/MyDrive/dataset/selada_hijau/FNNPK/-K/\"\n",
        "\n",
        "TRAINING_DIR = \"/content/drive/MyDrive/dataset/selada_hijau/training\"\n",
        "VALIDATION_DIR = \"/content/drive/MyDrive/dataset/selada_hijau/validation/\"\n",
        "\n",
        "TRAINING_FN_DIR = os.path.join(TRAINING_DIR, \"FN/\")\n",
        "TRAINING_N_DIR = os.path.join(TRAINING_DIR, \"N/\")\n",
        "TRAINING_P_DIR = os.path.join(TRAINING_DIR, \"P/\")\n",
        "TRAINING_K_DIR = os.path.join(TRAINING_DIR, \"K/\")\n",
        "\n",
        "VALIDATION_FN_DIR = os.path.join(VALIDATION_DIR, \"FN/\")\n",
        "VALIDATION_N_DIR = os.path.join(VALIDATION_DIR, \"N/\")\n",
        "VALIDATION_P_DIR = os.path.join(VALIDATION_DIR, \"P/\")\n",
        "VALIDATION_K_DIR = os.path.join(VALIDATION_DIR, \"K/\")\n",
        "\n",
        "\n",
        "# Define proportion of images used for training\n",
        "split_size = .9\n",
        "\n",
        "# Run the function\n",
        "# NOTE: Messages about zero length images should be printed out\n",
        "split_data(FN_DIR, TRAINING_FN_DIR, VALIDATION_FN_DIR, split_size)\n",
        "split_data(N_DIR, TRAINING_N_DIR, VALIDATION_N_DIR, split_size)\n",
        "split_data(P_DIR, TRAINING_P_DIR, VALIDATION_P_DIR, split_size)\n",
        "split_data(K_DIR, TRAINING_K_DIR, VALIDATION_K_DIR, split_size)\n",
        "\n",
        "# Check that the number of images matches the expected output\n",
        "\n",
        "# Your function should perform copies rather than moving images so original directories should contain unchanged images\n",
        "print(f\"\\n\\nOriginal cat's directory has {len(os.listdir(FN_DIR))} images\")\n",
        "print(f\"Original dog's directory has {len(os.listdir(N_DIR))} images\\n\")\n",
        "print(f\"\\n\\nOriginal cat's directory has {len(os.listdir(P_DIR))} images\")\n",
        "print(f\"Original dog's directory has {len(os.listdir(K_DIR))} images\\n\")\n",
        "\n",
        "# Training and validation splits\n",
        "print(f\"There are {len(os.listdir(TRAINING_FN_DIR))} images of FN for training\")\n",
        "print(f\"There are {len(os.listdir(TRAINING_N_DIR))} images of N for training\")\n",
        "print(f\"There are {len(os.listdir(TRAINING_P_DIR))} images of P for training\")\n",
        "print(f\"There are {len(os.listdir(TRAINING_K_DIR))} images of K for training\")\n",
        "\n",
        "print(f\"There are {len(os.listdir(VALIDATION_FN_DIR))} images of  FN validation\")\n",
        "print(f\"There are {len(os.listdir(VALIDATION_N_DIR))} images of N for validation\")\n",
        "print(f\"There are {len(os.listdir(VALIDATION_P_DIR))} images of  P validation\")\n",
        "print(f\"There are {len(os.listdir(VALIDATION_K_DIR))} images of K for validation\")"
      ],
      "metadata": {
        "id": "RxACWxK1uXm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "TRAINING_DIR = \"/content/drive/MyDrive/dataset/selada_hijau/training\"\n",
        "training_datagen = ImageDataGenerator(\n",
        "      rescale = 1./255,\n",
        "\t    rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "VALIDATION_DIR = \"/content/drive/MyDrive/dataset/selada_hijau/validation\"\n",
        "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "train_generator = training_datagen.flow_from_directory(\n",
        "\tTRAINING_DIR,\n",
        "\ttarget_size=(150,150),\n",
        "\tclass_mode='categorical',\n",
        "  batch_size=28\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "\tVALIDATION_DIR,\n",
        "\ttarget_size=(150,150),\n",
        "\tclass_mode='categorical',\n",
        "  batch_size=28\n",
        ")"
      ],
      "metadata": {
        "id": "LNVGNxMKovDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "def create_model():\n",
        "  model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fourth convolution\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(4, activation='softmax')\n",
        "  ])\n",
        "  # Set the training parameters\n",
        "  model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "pbV5kQ16y-Er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the untrained model\n",
        "model = create_model()\n",
        "\n",
        "# Train the model\n",
        "# Note that this may take some time.\n",
        "history = model.fit(train_generator,\n",
        "                    epochs=15,\n",
        "                    verbose=1,\n",
        "                    validation_data=validation_generator)"
      ],
      "metadata": {
        "id": "VyTSNQ8TygRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the results\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "epochs = range(len(loss))\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uT7xC9wl0bk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SV5KcjUI4EaU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}